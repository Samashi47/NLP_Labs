{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.number import ArNumbers\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "import qalsadi.lemmatizer as lem\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'}\n",
    "result = requests.get('https://www.hespress.com/economie', headers=headers)\n",
    "doc = BeautifulSoup(result.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the divs that contain the links to articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div = doc.find_all(name='div',attrs={'class':'posts-categoy'})\n",
    "posts = main_div[0].find_all(name='div',attrs={'class':'card-img-top'})\n",
    "articals = []\n",
    "for article in posts:\n",
    "    # links\n",
    "    link = article.find(name='a').get('href')\n",
    "    # article titles\n",
    "    title = article.find(name='a').get('title')\n",
    "    articals.append({title : link})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing links and titles in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hess_article_links.json', 'w', encoding='utf-8') as f:\n",
    "    data = []\n",
    "    for article in articals:\n",
    "        title = list(article.keys())[0]\n",
    "        link = list(article.values())[0]\n",
    "        data.append({'title': title, 'link': link})\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the divs that contain the links to articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hess_article_links.json', 'r', encoding='utf-8') as f:\n",
    "    article_content = []\n",
    "    data = json.load(f)\n",
    "    for article in data:\n",
    "        article_result = requests.get(article['link'], headers=headers)\n",
    "        article_doc = BeautifulSoup(article_result.text, 'html.parser')\n",
    "        article_content.append(article_doc.find_all(name='div',attrs={'class':'article-content'})[0].find_all(name='p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing articles and their respective titles in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hess_article_content.json', 'w', encoding='utf-8') as f:\n",
    "    contents = []\n",
    "    for i,content in enumerate(article_content):\n",
    "        content = ''\n",
    "        for p in article_content[i]:\n",
    "            content += p.text\n",
    "        contents.append({'title': list(data[i].values())[0], 'content': content})\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the two json files in a MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hess_article_links.json', 'r', encoding='utf-8') as f1:\n",
    "    links = json.load(f1)\n",
    "with open('hess_article_content.json', 'r', encoding='utf-8') as f2:\n",
    "    art_content = json.load(f2)\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['NLP_lab1']\n",
    "coll1 = db['hess_article_links']\n",
    "coll2 = db['hess_article_content']\n",
    "coll1.drop()\n",
    "coll1.insert_many(links)\n",
    "coll2.drop()\n",
    "coll2.insert_many(art_content)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the first article for the NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:  أعلنت شركة الطيران الإيرلندية “ريان إير” أنها نقلت 13.6 مليون مسافر في شهر مارس الماضي، بزيادة قدرها 8 في المائة مقارنة بشهر مارس 2023.وقامت الشركة بتشغيل 77 ألف رحلة خلال شهر مارس بنسبة ملء وصلت 93 في المائة، موضحة أن 950 رحلة ألغيت بسبب النزاعات المسلحة.وأشارت الشركة إلى أنها نقلت، بين مارس 2023 ومارس 2024، ما مجموعه 183.7 مليون مسافر (+9 في المائة مقارنة بالفترة نفسها من العام السابق) بمعدل ملء قدره 94 في المائة (+1 نقطة).\n"
     ]
    }
   ],
   "source": [
    "text = art_content[0]['content']\n",
    "text.encode('utf-8')\n",
    "print(\"Raw text: \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenized list:  ['أعلنت', 'شركة', 'الطيران', 'الإيرلندية', '“', 'ريان', 'إير', '”', 'أنها', 'نقلت', '13', '.', '6', 'مليون', 'مسافر', 'في', 'شهر', 'مارس', 'الماضي', '،', 'بزيادة', 'قدرها', '8', 'في', 'المائة', 'مقارنة', 'بشهر', 'مارس', '2023', '.', 'وقامت', 'الشركة', 'بتشغيل', '77', 'ألف', 'رحلة', 'خلال', 'شهر', 'مارس', 'بنسبة', 'ملء', 'وصلت', '93', 'في', 'المائة', '،', 'موضحة', 'أن', '950', 'رحلة', 'ألغيت', 'بسبب', 'النزاعات', 'المسلحة', '.', 'وأشارت', 'الشركة', 'إلى', 'أنها', 'نقلت', '،', 'بين', 'مارس', '2023', 'ومارس', '2024', '،', 'ما', 'مجموعه', '183', '.', '7', 'مليون', 'مسافر', '(+', '9', 'في', 'المائة', 'مقارنة', 'بالفترة', 'نفسها', 'من', 'العام', 'السابق', ')', 'بمعدل', 'ملء', 'قدره', '94', 'في', 'المائة', '(+', '1', 'نقطة', ').']\n",
      "Sentence tokenized list:  ['أعلنت شركة الطيران الإيرلندية “ريان إير” أنها نقلت 13.6 مليون مسافر في شهر مارس الماضي،', 'بزيادة قدرها 8 في المائة مقارنة بشهر مارس 2023.وقامت الشركة بتشغيل 77 ألف رحلة خلال شهر مارس بنسبة ملء وصلت 93 في المائة،', 'موضحة أن 950 رحلة ألغيت بسبب النزاعات المسلحة.وأشارت الشركة إلى أنها نقلت،', 'بين مارس 2023 ومارس 2024،', 'ما مجموعه 183.7 مليون مسافر (+9 في المائة مقارنة بالفترة نفسها من العام السابق) بمعدل ملء قدره 94 في المائة (+1 نقطة).']\n",
      "Word tokenized list length:  95\n",
      "Sentence tokenized list length:  5\n"
     ]
    }
   ],
   "source": [
    "tokens = araby.tokenize(text)\n",
    "sent_tokens = araby.sentence_tokenize(text)\n",
    "print(\"Word tokenized list: \", tokens)\n",
    "print(\"Sentence tokenized list: \", sent_tokens)\n",
    "print(\"Word tokenized list length: \", len(tokens))\n",
    "print(\"Sentence tokenized list length: \", len(sent_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_punct = ''')(+`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”،.”…“–ـ”.'''\n",
    "en_punct = string.punctuation\n",
    "punct_lst = ar_punct + en_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text without punctuation:  ['أعلنت', 'شركة', 'الطيران', 'الإيرلندية', 'ريان', 'إير', 'أنها', 'نقلت', '13', '6', 'مليون', 'مسافر', 'في', 'شهر', 'مارس', 'الماضي', 'بزيادة', 'قدرها', '8', 'في', 'المائة', 'مقارنة', 'بشهر', 'مارس', '2023', 'وقامت', 'الشركة', 'بتشغيل', '77', 'ألف', 'رحلة', 'خلال', 'شهر', 'مارس', 'بنسبة', 'ملء', 'وصلت', '93', 'في', 'المائة', 'موضحة', 'أن', '950', 'رحلة', 'ألغيت', 'بسبب', 'النزاعات', 'المسلحة', 'وأشارت', 'الشركة', 'إلى', 'أنها', 'نقلت', 'بين', 'مارس', '2023', 'ومارس', '2024', 'ما', 'مجموعه', '183', '7', 'مليون', 'مسافر', '9', 'في', 'المائة', 'مقارنة', 'بالفترة', 'نفسها', 'من', 'العام', 'السابق', 'بمعدل', 'ملء', 'قدره', '94', 'في', 'المائة', '1', 'نقطة', ').']\n",
      "Sentence tokenized text without punctuation:  ['أعلنت شركة الطيران الإيرلندية ريان إير أنها نقلت 136 مليون مسافر في شهر مارس الماضي', 'بزيادة قدرها 8 في المائة مقارنة بشهر مارس 2023وقامت الشركة بتشغيل 77 ألف رحلة خلال شهر مارس بنسبة ملء وصلت 93 في المائة', 'موضحة أن 950 رحلة ألغيت بسبب النزاعات المسلحةوأشارت الشركة إلى أنها نقلت', 'بين مارس 2023 ومارس 2024', 'ما مجموعه 1837 مليون مسافر 9 في المائة مقارنة بالفترة نفسها من العام السابق بمعدل ملء قدره 94 في المائة 1 نقطة']\n",
      "Length of tokenized text without punctuation:  82\n",
      "Length of sentence tokenized text without punctuation:  5\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in tokens if token not in punct_lst]\n",
    "sent_tokens = [sent.translate(str.maketrans('', '', punct_lst)) for sent in sent_tokens]\n",
    "print(\"Tokenized text without punctuation: \",tokens)\n",
    "print(\"Sentence tokenized text without punctuation: \",sent_tokens)\n",
    "print(\"Length of tokenized text without punctuation: \", len(tokens))\n",
    "print(\"Length of sentence tokenized text without punctuation: \", len(sent_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:  {'أنشأ', 'بماذا', 'كيت', 'فيما', 'إمّا', 'عاشر', 'أقبل', 'جنيه', 'سين', 'هَذِي', 'بهن', 'بين', 'ذِي', 'هَاتِه', 'لهم', 'بس', 'ثمانية', 'تسعمئة', 'هلم', 'شباط', 'آ', 'إذ', 'غالبا', 'أفٍّ', 'مكانكم', 'حيثما', 'قاطبة', 'اثنا', 'أيّان', 'كما', 'طالما', 'هيهات', 'ف', 'تاسع', 'خمس', 'أمام', 'حيث', 'بَلْهَ', 'حبذا', 'مكانَك', 'الألى', 'تانِ', 'ة', 'ثالث', 'اللاتي', 'فإذا', 'ح', 'شتانَ', 'لكيلا', 'إذاً', 'ثمان', 'كأن', 'إياها', 'كأنما', 'أيضا', 'بغتة', 'أوت', 'ذَيْنِ', 'دواليك', 'أمسى', 'سوى', 'س', 'صبر', 'هَذانِ', 'دونك', 'مه', 'اللتيا', 'لنا', 'لكم', 'قبل', 'قلما', 'بك', 'إنَّ', 'معاذ', 'سبعة', 'منها', 'نَّ', 'ثمّ', 'ليت', 'اربعين', 'هذا', 'نوفمبر', 'ته', 'أما', 'آنفا', 'هَذَيْنِ', 'إلا', 'إحدى', 'تعسا', 'مائة', 'حقا', 'جيم', 'تانِك', 'إي', 'هكذا', 'لن', 'زعم', 'هَيْهات', 'يفعلان', 'أجمع', 'ممن', 'خ', 'أربعمئة', 'هذين', 'هما', 'حدَث', 'لستن', 'لهما', 'م', 'ى', 'حتى', 'حَذارِ', 'جويلية', 'صهْ', 'آناء', 'فاء', 'فوق', 'كلّما', 'درى', 'رجع', 'بهم', 'هنا', 'إياهما', 'هلّا', 'اللتان', 'ظاء', 'صار', 'ارتدّ', 'وإذا', 'كذا', 'وإذ', 'ذات', 'كاف', 'لعمر', 'تلكما', 'مذ', 'شمال', 'راء', 'لا', 'لسنا', 'أربعة', 'زود', 'تموز', 'تسعين', 'مما', 'اللائي', 'أحد', 'كليكما', 'كسا', 'ط', 'تينك', 'هيّا', 'هبّ', 'أسكن', 'أل', 'هاتين', 'ليست', 'حاي', 'مافتئ', 'هَاتَيْنِ', 'الذي', 'واحد', 'ساء', 'مهما', 'كأي', 'ظنَّ', 'بها', 'غير', 'في', 'ث', 'تي', 'سرا', 'نحو', 'إذا', 'حاء', 'هل', 'ذانك', 'خاء', 'غ', 'إلَيْكَ', 'دون', 'كلَّا', 'أف', 'هم', 'أى', 'لدن', 'هلا', 'ر', 'حسب', 'آهٍ', 'بَسْ', 'لولا', 'لو', 'بسّ', 'هي', 'حمدا', 'ثان', 'كليهما', 'من', 'ألا', 'آي', 'إليكنّ', 'ذه', 'ستمائة', 'إياكن', 'بما', 'عجبا', 'قرش', 'إياي', 'بكما', 'فلان', 'أيلول', 'ثمنمئة', 'أ', 'أولاء', 'وإن', 'حزيران', 'ز', 'تِه', 'ريال', 'إياكم', 'تين', 'بهما', 'أول', 'بعدا', 'ذوا', 'اثنين', 'أم', 'إما', 'واو', 'ش', 'الألاء', 'اللذين', 'ذاك', 'إياكما', 'إى', 'ليس', 'تلك', 'بكن', 'ولا', 'أمامكَ', 'عن', 'هاتان', 'ستمئة', 'علم', 'إلّا', 'ثماني', 'فيفري', 'كلا', 'ء', 'أمّا', 'أغسطس', 'وهو', 'طاء', 'صدقا', 'يوان', 'حبيب', 'مرّة', 'جلل', 'جانفي', 'أخٌ', 'سنتيم', 'ذهب', 'ومن', 'كى', 'والذي', 'تلكم', 'تشرين', 'أين', 'مئة', 'ولو', 'ي', 'والذين', 'ذلكما', 'ليرة', 'ذي', 'اللتين', 'بخ', 'زاي', 'ءَ', 'د', 'ذلك', 'أخبر', 'همزة', 'يمين', 'تفعلان', 'أي', 'أبو', 'خبَّر', 'هاته', 'نَخْ', 'سحقا', 'مادام', 'أوه', 'جعل', 'أفعل به', 'ذال', 'أنتِ', 'خامس', 'كأنّ', 'جميع', 'قاف', 'سبعمئة', 'ستين', 'أجل', 'عليك', 'فبراير', 'سوف', 'هذان', 'مكانكما', 'هَجْ', 'أيّ', 'شيكل', 'ثلاث', 'وهب', 'أولالك', 'خمسين', 'ذو', 'حاشا', 'كن', 'هاتي', 'بكم', 'آها', 'تارة', 'ذواتا', 'ضحوة', 'حجا', 'أيها', 'عند', 'لستم', 'ياء', 'سمعا', 'اخلولق', 'انبرى', 'نحن', 'ليستا', 'شتان', 'حيَّ', 'غين', 'ق', 'سبعين', 'اثني', 'منه', 'سبحان', 'أوشك', 'إياه', 'مساء', 'مارس', 'قد', 'خاصة', 'أقل', 'آب', 'ذلكم', 'مثل', 'دينار', 'ا', 'انقلب', 'وَيْ', 'خمسة', 'ه', 'هيت', 'كيفما', 'شَتَّانَ', 'سبتمبر', 'بنا', 'فضلا', 'ؤ', 'كلاهما', 'ليسوا', 'ج', 'بئس', 'عشرون', 'راح', 'ابتدأ', 'أربعاء', 'درهم', 'لدى', 'إنما', 'اثنان', 'هللة', 'حمٌ', 'دولار', 'استحال', 'علًّ', 'أربعمائة', 'أيا', 'ها', 'كثيرا', 'ثمانمئة', 'لكما', 'أفريل', 'رابع', 'ثمَّ', 'عوض', 'سبع', 'نيسان', 'إليك', 'يونيو', 'بيد', 'هاكَ', 'بمن', 'نيف', 'بات', 'عدَّ', 'هذه', 'إيانا', 'أوّهْ', 'فيم', 'بضع', 'لام', 'إياك', 'هو', 'اربعون', 'أنا', 'ثامن', 'شبه', 'يوليو', 'ذينك', 'نون', 'إزاء', 'سادس', 'كم', 'وراءَك', 'سرعان', 'كذلك', 'ذلكن', 'لي', 'أنت', 'عيانا', 'لم', 'مايو', 'مئتان', 'لئن', 'منذ', 'إلى', 'يورو', 'عشرة', 'ريث', 'تفعلون', 'تسعة', 'ذِه', 'الذين', 'رزق', 'و', 'سقى', 'بخٍ', 'عامة', 'أكثر', 'خمسون', 'هيا', 'أبريل', 'آهِ', 'عين', 'فلا', 'عليه', 'عاد', 'نا', 'إذن', 'رأى', 'عما', 'أهلا', 'لا سيما', 'لمّا', 'ذ', 'أعلم', 'ظلّ', 'فيه', 'كرب', 'سبعون', 'حرى', 'هَذِه', 'ب', 'خال', 'أعطى', 'ما', 'ما برح', 'حادي', 'كل', 'عشرين', 'إليكن', 'وجد', 'هن', 'ثلاثمائة', 'صباح', 'ثمانون', 'هَاتِي', 'ع', 'خلف', 'يناير', 'ذين', 'أبٌ', 'كأيّ', 'ثلاثمئة', 'لست', 'هَاتانِ', 'أنًّ', 'ّأيّان', 'ثلاثة', 'ن', 'رُبَّ', 'سبعمائة', 'لما', 'صبرا', 'أضحى', 'ظ', 'أنّى', 'تجاه', 'عَدَسْ', 'أبدا', 'لكنَّ', 'أنتن', 'رويدك', 'ألفى', 'عشر', 'إليكما', 'هؤلاء', 'ذا', 'لك', 'هاء', 'هاك', 'أولئك', 'لوما', 'أربع', 'حار', 'ثلاثون', 'ألف', 'إنه', 'صهٍ', 'صراحة', 'كلما', 'تعلَّم', 'تَيْنِ', 'لبيك', 'إليكم', 'ثلاثين', 'بل', 'إذما', 'أنى', 'بعض', 'كان', 'ثاني', 'فرادى', 'مازال', 'حين', 'إليكَ', 'خمسمئة', 'أصلا', 'أمس', 'كاد', 'ثلاثاء', 'ست', 'ذانِ', 'الآن', 'شين', 'ستة', 'إن', 'كيف', 'أمامك', 'لها', 'ذان', 'ما انفك', 'جير', 'مليم', 'وما', 'كي', 'آمينَ', 'أيار', 'نعم', 'بؤسا', 'لستما', 'جوان', 'عل', 'يفعلون', 'أنتما', 'له', 'مع', 'ص', 'يا', 'أينما', 'لكي', 'ماي', 'بلى', 'ض', 'عسى', 'ليسا', 'ثم', 'أخذ', 'لهن', 'ين', 'لسن', 'خلا', 'لكنما', 'تسعون', 'غدا', 'شرع', 'لعل', 'ترك', 'أنبأ', 'نبَّا', 'كلتا', 'آهاً', 'هَذا', 'تلقاء', 'ئ', 'أخو', 'قطّ', 'متى', 'علق', 'ثاء', 'وُشْكَانَ', 'باء', 'هناك', 'ميم', 'فإن', 'صاد', 'مكانكنّ', 'به', 'نفس', 'جمعة', 'إيهٍ', 'لكن', 'ديسمبر', 'وا', 'ورد', 'أصبح', 'كانون', 'فلس', 'كِخ', 'ثمّة', 'التي', 'تسعمائة', 'حمو', 'ماذا', 'عدا', 'ما أفعله', 'طرا', 'آذار', 'اتخذ', 'تخذ', 'هذي', 'فمن', 'أو', 'كأيّن', 'لات', 'سبت', 'سابع', 'إياهن', 'قام', 'إنا', 'ولكن', 'ذواتي', 'أرى', 'تحوّل', 'أنتم', 'تسع', 'ستون', 'أن', 'خمسمائة', 'لعلَّ', 'تِي', 'آه', 'هنالك', 'آض', 'إيه', 'على', 'غادر', 'ل', 'هَؤلاء', 'خلافا', 'ت', 'بي', 'هاهنا', 'أُفٍّ', 'اللواتي', 'أطعم', 'طفق', 'ذيت', 'بطآن', 'كأين', 'أمد', 'فيها', 'خميس', 'تفعلين', 'ك', 'تحت', 'دال', 'واهاً', 'تبدّل', 'تاء', 'أكتوبر', 'اللذان', 'طَق', 'ثمانين', 'لاسيما', 'فو', 'غداة', 'طاق', 'إياهم', 'ثمة', 'بعد', 'ضاد'}\n",
      "Tokenized text without stopwords:  ['أعلنت', 'شركة', 'الطيران', 'الإيرلندية', 'ريان', 'إير', 'أنها', 'نقلت', '13', '6', 'مليون', 'مسافر', 'شهر', 'الماضي', 'بزيادة', 'قدرها', '8', 'المائة', 'مقارنة', 'بشهر', '2023', 'وقامت', 'الشركة', 'بتشغيل', '77', 'رحلة', 'خلال', 'شهر', 'بنسبة', 'ملء', 'وصلت', '93', 'المائة', 'موضحة', '950', 'رحلة', 'ألغيت', 'بسبب', 'النزاعات', 'المسلحة', 'وأشارت', 'الشركة', 'أنها', 'نقلت', '2023', 'ومارس', '2024', 'مجموعه', '183', '7', 'مليون', 'مسافر', '9', 'المائة', 'مقارنة', 'بالفترة', 'نفسها', 'العام', 'السابق', 'بمعدل', 'ملء', 'قدره', '94', 'المائة', '1', 'نقطة', ').']\n",
      "Sentence tokenized text without stopwords:  ['أعلنت شركة الطيران الإيرلندية ريان إير أنها نقلت 136 مليون مسافر  شهر  الماضي', 'بزيادة قدرها 8  المائة مقارنة بشهر  2023وقامت الشركة بتشغيل 77  رحلة خلال شهر  بنسبة ملء وصلت 93  المائة', 'موضحة  950 رحلة ألغيت بسبب النزاعات المسلحةوأشارت الشركة  أنها نقلت', '  2023 ومارس 2024', ' مجموعه 1837 مليون مسافر 9  المائة مقارنة بالفترة نفسها  العام السابق بمعدل ملء قدره 94  المائة 1 نقطة']\n",
      "Length of tokenized text without stopwords:  67\n",
      "Length of sentence tokenized text without stopwords:  5\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "print(\"Stop words: \",stop_words)\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "sent_tokens = [''.join([x for x in re.split(r'(\\W+)', sent) if x not in stop_words]) for sent in sent_tokens]\n",
    "\n",
    "print(\"Tokenized text without stopwords: \",tokens)\n",
    "print(\"Sentence tokenized text without stopwords: \",sent_tokens)\n",
    "print(\"Length of tokenized text without stopwords: \", len(tokens))\n",
    "print(\"Length of sentence tokenized text without stopwords: \", len(sent_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number to string:  ['أعلنت', 'شركة', 'الطيران', 'الإيرلندية', 'ريان', 'إير', 'أنها', 'نقلت', 'ثلاث عشرة', 'ست', 'مليون', 'مسافر', 'شهر', 'الماضي', 'بزيادة', 'قدرها', 'ثماني', 'المائة', 'مقارنة', 'بشهر', 'ألفان و ثلاث و عشرون', 'وقامت', 'الشركة', 'بتشغيل', 'سبع و سبعون', 'رحلة', 'خلال', 'شهر', 'بنسبة', 'ملء', 'وصلت', 'ثلاث و تسعون', 'المائة', 'موضحة', 'تسعمئة و خمسون', 'رحلة', 'ألغيت', 'بسبب', 'النزاعات', 'المسلحة', 'وأشارت', 'الشركة', 'أنها', 'نقلت', 'ألفان و ثلاث و عشرون', 'ومارس', 'ألفان و أربع و عشرون', 'مجموعه', 'مئة و ثلاث و ثمانون', 'سبع', 'مليون', 'مسافر', 'تسع', 'المائة', 'مقارنة', 'بالفترة', 'نفسها', 'العام', 'السابق', 'بمعدل', 'ملء', 'قدره', 'أربع و تسعون', 'المائة', 'واحد', 'نقطة', ').']\n",
      "Numerical tokens:  ['ثلاث عشرة', 'ست', 'ثماني', 'ألفان و ثلاث و عشرون', 'سبع و سبعون', 'ثلاث و تسعون', 'تسعمئة و خمسون', 'ألفان و ثلاث و عشرون', 'ألفان و أربع و عشرون', 'مئة و ثلاث و ثمانون', 'سبع', 'تسع', 'أربع و تسعون', 'واحد']\n"
     ]
    }
   ],
   "source": [
    "arnum = ArNumbers()\n",
    "num_tokens = []\n",
    "for i,token in enumerate(tokens):\n",
    "    if tokens[i].isdigit():\n",
    "        tokens[i] = arnum.int2str(tokens[i])\n",
    "        num_tokens.append(tokens[i])\n",
    "print(\"Number to string: \", tokens)\n",
    "print(\"Numerical tokens: \", num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized token list:  ['اعلنت', 'شركه', 'الطيران', 'الايرلنديه', 'ريان', 'اير', 'انها', 'نقلت', 'مليون', 'مسافر', 'شهر', 'الماضي', 'بزياده', 'قدرها', 'الماءه', 'مقارنه', 'بشهر', 'وقامت', 'الشركه', 'بتشغيل', 'رحله', 'خلال', 'شهر', 'بنسبه', 'ملء', 'وصلت', 'الماءه', 'موضحه', 'رحله', 'الغيت', 'بسبب', 'النزاعات', 'المسلحه', 'واشارت', 'الشركه', 'انها', 'نقلت', 'ومارس', 'مجموعه', 'مليون', 'مسافر', 'الماءه', 'مقارنه', 'بالفتره', 'نفسها', 'العام', 'السابق', 'بمعدل', 'ملء', 'قدره', 'الماءه', 'نقطه', ').']\n"
     ]
    }
   ],
   "source": [
    "ArListem = ArabicLightStemmer()\n",
    "# removing numerical tokens\n",
    "for i in num_tokens:\n",
    "    tokens.remove(i)\n",
    "tokens = [ArListem.normalize(token) for token in tokens]\n",
    "print(\"Normalized token list: \", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed token list:  ['اعلن', 'شرك', 'طير', 'الايرلنديه', 'ري', 'ير', 'نه', 'قلت', 'ملي', 'مسافر', 'شهر', 'ماض', 'زياد', 'قدر', 'ماءه', 'مقار', 'شهر', 'قام', 'شركه', 'تشغيل', 'رحل', 'خلال', 'شهر', 'نسب', 'ملء', 'صل', 'ماءه', 'موضح', 'رحل', 'غيت', 'سبب', 'نزاع', 'مسلحه', 'اشار', 'شركه', 'نه', 'قلت', 'مارس', 'مجموع', 'ملي', 'مسافر', 'ماءه', 'مقار', 'فتره', 'نفس', 'عام', 'سابق', 'معدل', 'ملء', 'قدر', 'ماءه', 'قط', ').']\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = [ArListem.light_stem(token) for token in tokens]\n",
    "print(\"Stemmed token list: \", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized token list:  ['اعلنت', 'شرك', 'طير', 'الايرلنديه', 'ري', 'وري', 'انها', 'نقل', 'مليون', 'مسافر', 'شهر', 'ماضي', 'زياد', 'قدر', 'الماءه', 'مقارن', 'شهر', 'قام', 'الشركه', 'تشغيل', 'رحل', 'خلال', 'شهر', 'نسب', 'ملء', 'صلت', 'الماءه', 'موضح', 'رحل', 'الغيت', 'سبب', 'نزاع', 'المسلحه', 'واشارت', 'الشركه', 'انها', 'نقل', 'مارس', 'مجموع', 'مليون', 'مسافر', 'الماءه', 'مقارن', 'بالفتره', 'نفس', 'عام', 'سابق', 'معدل', 'ملء', 'قدر', 'الماءه', 'نقط', ').']\n"
     ]
    }
   ],
   "source": [
    "lemmer = lem.Lemmatizer()\n",
    "lemmatized_tokens = [lemmer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized token list: \", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs lemmatization comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token list:  ['اعلنت', 'شركه', 'الطيران', 'الايرلنديه', 'ريان', 'اير', 'انها', 'نقلت', 'مليون', 'مسافر', 'شهر', 'الماضي', 'بزياده', 'قدرها', 'الماءه', 'مقارنه', 'بشهر', 'وقامت', 'الشركه', 'بتشغيل', 'رحله', 'خلال', 'شهر', 'بنسبه', 'ملء', 'وصلت', 'الماءه', 'موضحه', 'رحله', 'الغيت', 'بسبب', 'النزاعات', 'المسلحه', 'واشارت', 'الشركه', 'انها', 'نقلت', 'ومارس', 'مجموعه', 'مليون', 'مسافر', 'الماءه', 'مقارنه', 'بالفتره', 'نفسها', 'العام', 'السابق', 'بمعدل', 'ملء', 'قدره', 'الماءه', 'نقطه', ').']\n",
      "Stemmed token list:  ['اعلن', 'شرك', 'طير', 'الايرلنديه', 'ري', 'ير', 'نه', 'قلت', 'ملي', 'مسافر', 'شهر', 'ماض', 'زياد', 'قدر', 'ماءه', 'مقار', 'شهر', 'قام', 'شركه', 'تشغيل', 'رحل', 'خلال', 'شهر', 'نسب', 'ملء', 'صل', 'ماءه', 'موضح', 'رحل', 'غيت', 'سبب', 'نزاع', 'مسلحه', 'اشار', 'شركه', 'نه', 'قلت', 'مارس', 'مجموع', 'ملي', 'مسافر', 'ماءه', 'مقار', 'فتره', 'نفس', 'عام', 'سابق', 'معدل', 'ملء', 'قدر', 'ماءه', 'قط', ').']\n",
      "Lemmatized token list:  ['اعلنت', 'شرك', 'طير', 'الايرلنديه', 'ري', 'وري', 'انها', 'نقل', 'مليون', 'مسافر', 'شهر', 'ماضي', 'زياد', 'قدر', 'الماءه', 'مقارن', 'شهر', 'قام', 'الشركه', 'تشغيل', 'رحل', 'خلال', 'شهر', 'نسب', 'ملء', 'صلت', 'الماءه', 'موضح', 'رحل', 'الغيت', 'سبب', 'نزاع', 'المسلحه', 'واشارت', 'الشركه', 'انها', 'نقل', 'مارس', 'مجموع', 'مليون', 'مسافر', 'الماءه', 'مقارن', 'بالفتره', 'نفس', 'عام', 'سابق', 'معدل', 'ملء', 'قدر', 'الماءه', 'نقط', ').']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original token list: \", tokens)\n",
    "print(\"Stemmed token list: \", stemmed_tokens)\n",
    "print(\"Lemmatized token list: \", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning approach for PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = FarasaPOSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged S/S ال+ ايرلنديه/DET+NOUN-MS E/E\n"
     ]
    }
   ],
   "source": [
    "tagged_text =  pos_tagger.tag(tokens[3]) # [pos_tagger.tag(token) for token in lemmatized_tokens]\n",
    "print(\"POS Tagged\",tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based approach for PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "def tag(token):\n",
    "    rules_1_2 = [r'^\\u0643\\u0627\\u0644.*$', r'^\\u0628\\u0627\\u0644.*$', r'^\\u0641\\u0627\\u0644.*$', \n",
    "                r'^\\u0648\\u0627\\u0644.*$',r'\\w*\\u0627\\u0626\\u0647\\b', r'\\w*\\u0627\\u0626\\u0643\\b',\n",
    "                r'\\w*\\u0627\\u0626\\u064a\\b', r'\\w*\\u0627\\u0624\\u0643\\b', r'\\w*\\u0627\\u0624\\u0647\\b',\n",
    "                r'\\w*\\u0627\\u0621\\u0643\\b',r'\\w*\\u0627\\u0621\\u0647\\b',r'\\w*\\u0647\\u0645\\u0627\\b',\n",
    "                r'\\w*\\u0643\\u0645\\u0627\\b']\n",
    "    rules_3_4 = [r'^\\u0633\\u064a.*$', r'^\\u0633\\u062a.*$', r'^\\u0633\\u0646.*$', r'^\\u0633\\u0623.*$',\n",
    "                r'^\\u0633\\u0627.*$',r'^\\u0644\\u0627.*$',r'^\\u0644\\u0623.*$',r'^\\u0644\\u0646.*$',\n",
    "                r'^\\u0644\\u062a.*$',r'^\\u0644\\u064a.*$', r'\\w*\\u064a\\b',r'\\w*\\u0647\\b',r'\\w*\\u0643\\b',\n",
    "                r'\\w*\\u0627\\b',r'\\w*\\u0646\\b',r'\\w*\\u0648\\b']\n",
    "    rules_5_6 = [r'^\\w{3}.*\\u0649$', r'^\\w{2}\\u0648\\w{1}$',r'^\\w{2}\\u0627\\u0621$',r'^\\w*\\u0627\\u062a$']\n",
    "    rule_7 = r'^(\\u064a|\\u0646)\\w*(\\u0648\\u0646|\\u064a\\u0646)$'\n",
    "    rules_8_9 = [r'^[^(\\u064a|\\u0646)]\\w*(\\u0648\\u0646|\\u064a\\u0646)$']\n",
    "    if any(re.match(pattern, token) for pattern in rules_1_2):\n",
    "        return 'NOUN'\n",
    "    elif any(re.match(pattern, token) for pattern in rules_3_4):\n",
    "        return 'VERB'\n",
    "    elif any(re.match(pattern, token) for pattern in rules_5_6):\n",
    "        return 'NOUN'\n",
    "    elif re.match(rule_7, token):\n",
    "        return 'VERB'\n",
    "    elif any(re.match(pattern, token) for pattern in rules_8_9):\n",
    "        return 'NOUN'\n",
    "    \n",
    "print(tag('فعول'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Zerrouki, T., (2023). PyArabic: A Python package for Arabic text. Journal of Open Source Software, 8(84), 4886, https://doi.org/10.21105/joss.04886\n",
    "- Alkhatib, R. M., Zerrouki, T., Shquier, M. M. A., & Balla, A. (2023). Tashaphyne0.4: A new arabic light stemmer based on rhyzome modeling approach. Information Retrieval Journa, 26(14). doi: https://doi.org/10.1007/s10791-023-09429-y\n",
    "- T. Zerrouki, Qalsadi, Arabic mophological analyzer Library for python.,  https://pypi.python.org/pypi/qalsadi/\n",
    "- Hegazi, M. O., Al-Dossari, Y., Al-Yahy, A., Al-Sumari, A., & Hilal, A. (2021). Preprocessing Arabic text on social media. Heliyon, 7(2), e06191. doi:10.1016/j.heliyon.2021.e06191 \n",
    "- Sawalha, M., Atwell, E., & Abushariah, M. A. M. (2013). SALMA: Standard Arabic Language Morphological Analysis. 2013 1st International Conference on Communications, Signal Processing, and Their Applications (ICCSPA). doi:10.1109/iccspa.2013.6487311 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
